\graphicspath{ {images/} }

\titledquestion{Analyzing NMT Systems}[25]

\begin{parts}

    \part[3] Look at the {\monofam{src.vocab}} file for some examples of phrases and words in the source language vocabulary. When encoding an input Mandarin Chinese sequence into ``pieces'' in the vocabulary, the tokenizer maps the sequence to a series of vocabulary items, each consisting of one or more characters (thanks to the {\monofam{sentencepiece}} tokenizer, we can perform this segmentation even when the original text has no white space). Given this information, how could adding a 1D Convolutional layer after the embedding layer and before passing the embeddings into the bidirectional encoder help our NMT system? \textbf{Hint:} each Mandarin Chinese character is either an entire word or a morpheme in a word. Look up the meanings of 电, 脑, and 电脑 separately for an example. The characters 电 (electricity) and  脑 (brain) when combined into the phrase 电脑 mean computer.

    \ifans{Because each words in Chinese are not separated by white spacce. Every character in Chinese is a word and their morpheme can also be a word such as 电, 脑 and 电脑, etc. and they represent different meaning. The 1D Convotional layer help us separate each words properly, then every tokenlized words's meaning are same as what they are in sentence. By doing these, we can capture patterns in Chinese sentences better.}


    \part[8] Here we present a series of errors we found in the outputs of our NMT model (which is the same as the one you just trained). For each example of a reference (i.e., `gold') English translation, and NMT (i.e., `model') English translation, please:
    
    \begin{enumerate}
        \item Identify the error in the NMT translation.
        \item Provide possible reason(s) why the model may have made the error (either due to a specific linguistic construct or a specific model limitation).
        \item Describe one possible way we might alter the NMT system to fix the observed error. There are more than one possible fixes for an error. For example, it could be tweaking the size of the hidden layers or changing the attention mechanism.
    \end{enumerate}
    
    Below are the translations that you should analyze as described above. Only analyze the underlined error in each sentence. Rest assured that you don't need to know Mandarin to answer these questions. You just need to know English! If, however, you would like some additional color on the source sentences, feel free to use a resource like \url{https://www.archchinese.com/chinese_english_dictionary.html} to look up words. Feel free to search the training data file to have a better sense of how often certain characters occur.

    \begin{subparts}
        \subpart[2]
        \textbf{Source Sentence:} 贼人其后被警方拘捕及被判处盗窃罪名成立。 \newline
        \textbf{Reference Translation:} \textit{\underline{the culprits were} subsequently arrested and convicted.}\newline
        \textbf{NMT Translation:} \textit{\underline{the culprit was} subsequently arrested and sentenced to theft.}
        
        \ifans{
            \begin{enumerate}
                \item The reference translation is plural form but singular as the NMT translation. 
                \item The source sentence in Chinese didn't show the form clearly. We can't decide whether it is plural or singular. It is a specific linguistic construct. In fact, the plural form is more common in normal life. What's more, the training data may have higher frequency in singular nouns and the model may lack of attention in nouns.
                \item The specific linguistic problem are hard to avoid. But what we can do is to increase the frequency of plural nouns in data and to put more weight on the number of nouns.
            \end{enumerate}
         }


        \subpart[2]
        \textbf{Source Sentence}: 几乎已经没有地方容纳这些人,资源已经用尽。\newline
        \textbf{Reference Translation}: \textit{there is almost no space to accommodate these people, and resources have run out.   }\newline
        \textbf{NMT Translation}: \textit{the resources have been exhausted and \underline{resources have been exhausted}.}
        
        \ifans{
            \begin{enumerate}
                \item the former half part of source sentence is missed and the latter half part is repeated twice.
                \item This is obviously a wrong because of model limitation. The reason may be the two part in Chinese are emphasising the same thing that the resource have been exhausted. The latter part is the reason of the former part but they are similar in meaning. What's more the latter part are put much more attention, the former part may be ignored.
                \item Put more source sentence cataining such hidden causal relationship in data. Moreover, adjust the attention mechanism to make a blanced attention to such sentences.
            \end{enumerate}
        }

        \subpart[2]
        \textbf{Source Sentence}: 当局已经宣布今天是国殇日。 \newline
        \textbf{Reference Translation}: \textit{authorities have announced \underline{a national mourning today.}}\newline
        \textbf{NMT Translation}: \textit{the administration has announced \underline{today's day.}}
        
        \ifans{
            \begin{enumerate}
                \item The meaning of ``国殇日'' is not translated properly. It should be a day to mourn.
                \item The word ``国殇'' may be a very rare word so the model can't capture it's meaning properly. However, the word ``today'' means ``今日'', which is a high frequency word. So NMT model mixed them.
                \item Broaden the scale of corpus to cantain more rare word.
            \end{enumerate}
        }
        
        \subpart[2] 
        \textbf{Source Sentence\footnote{This is a Cantonese sentence! The data used in this assignment comes from GALE Phase 3, which is a compilation of news written in simplified Chinese from various sources scraped from the internet along with their translations. For more details, see \url{https://catalog.ldc.upenn.edu/LDC2017T02}. }:} 俗语有云:``唔做唔错"。\newline
        \textbf{Reference Translation:} \textit{\underline{`` act not,  err not "}, so a saying goes.}\newline
        \textbf{NMT Translation:} \textit{as the saying goes, \underline{`` it's not wrong. "}}
        
        \ifans{
            \begin{enumerate}
                \item The meaning of Chinese old saying is wrang. NMT Translation ignored the meaning of ``唔做'', and only translate the meaning of ``唔错''.
                \item Chinese is a large and complex language, which owns flexible word order. What's more, the old Chinese and modern Chinese are different in many ways. Our corpus may mainly consist of modern language, not old language, which make the model hard to understand the old idioms. 
                \item Broaden the scale of corpus to cantain more old idioms.
            \end{enumerate}
        }
    \end{subparts}


    \part[14] BLEU score is the most commonly used automatic evaluation metric for NMT systems. It is usually calculated across the entire test set, but here we will consider BLEU defined for a single example.\footnote{This definition of sentence-level BLEU score matches the \texttt{sentence\_bleu()} function in the \texttt{nltk} Python package. Note that the NLTK function is sensitive to capitalization. In this question, all text is lowercased, so capitalization is irrelevant. \\ \url{http://www.nltk.org/api/nltk.translate.html\#nltk.translate.bleu_score.sentence_bleu}
    } 
    Suppose we have a source sentence $\bs$, a set of $k$ reference translations $\br_1,\dots,\br_k$, and a candidate translation $\bc$. To compute the BLEU score of $\bc$, we first compute the \textit{modified $n$-gram precision} $p_n$ of $\bc$, for each of $n=1,2,3,4$, where $n$ is the $n$ in \href{https://en.wikipedia.org/wiki/N-gram}{n-gram}:
    \begin{align}
        p_n = \frac{ \displaystyle \sum_{\text{ngram} \in \bc} \min \bigg( \max_{i=1,\dots,k} \text{Count}_{\br_i}(\text{ngram}), \enspace \text{Count}_{\bc}(\text{ngram}) \bigg) }{\displaystyle \sum_{\text{ngram}\in \bc} \text{Count}_{\bc}(\text{ngram})}
    \end{align}
     Here, for each of the $n$-grams that appear in the candidate translation $\bc$, we count the maximum number of times it appears in any one reference translation, capped by the number of times it appears in $\bc$ (this is the numerator). We divide this by the number of $n$-grams in $\bc$ (denominator). \newline 

    Next, we compute the \textit{brevity penalty} BP. Let $len(c)$ be the length of $\bc$ and let $len(r)$ be the length of the reference translation that is closest to $len(c)$ (in the case of two equally-close reference translation lengths, choose $len(r)$ as the shorter one). 
    \begin{align}
        BP = 
        \begin{cases}
            1 & \text{if } len(c) \ge len(r) \\
            \exp \big( 1 - \frac{len(r)}{len(c)} \big) & \text{otherwise}
        \end{cases}
    \end{align}
    Lastly, the BLEU score for candidate $\bc$ with respect to $\br_1,\dots,\br_k$ is:
    \begin{align}
        BLEU = BP \times \exp \Big( \sum_{n=1}^4 \lambda_n \log p_n \Big)
    \end{align}
    where $\lambda_1,\lambda_2,\lambda_3,\lambda_4$ are weights that sum to 1. The $\log$ here is natural log.
    \newline
    \begin{subparts}
        \subpart[5] Please consider this example: \newline
        Source Sentence $\bs$: \textbf{需要有充足和可预测的资源。} 
        \newline
        Reference Translation $\br_1$: \textit{resources have to be sufficient and they have to be predictable}
        \newline
        Reference Translation $\br_2$: \textit{adequate and predictable resources are required}
        
        NMT Translation $\bc_1$: there is a need for adequate and predictable resources
        
        NMT Translation $\bc_2$: resources be suﬀicient and predictable to
        
        Please compute the BLEU scores for $\bc_1$ and $\bc_2$. Let $\lambda_i=0.5$ for $i\in\{1,2\}$ and $\lambda_i=0$ for $i\in\{3,4\}$ (\textbf{this means we ignore 3-grams and 4-grams}, i.e., don't compute $p_3$ or $p_4$). When computing BLEU scores, show your work (i.e., show your computed values for $p_1$, $p_2$, $len(c)$, $len(r)$ and $BP$). Note that the BLEU scores can be expressed between 0 and 1 or between 0 and 100. The code is using the 0 to 100 scale while in this question we are using the \textbf{0 to 1} scale. Please round your responses to 3 decimal places. 
        \newline
        
        Which of the two NMT translations is considered the better translation according to the BLEU Score? Do you agree that it is the better translation?
        
        \ifans{
            For $c_1$, Firstly, comupute the  \textit{modified $n$-gram precision} $p_n$ of $\bc_1$, for each of $n=1,2$
            \begin{align*}
                p_1
                & = \frac{ \displaystyle \sum_{\text{1-gram} \in \bc} \min \bigg( \max_{i=1,2} \text{Count}_{\br_i}(\text{1-gram}), \enspace \text{Count}_{\bc_1}(\text{1-gram}) \bigg) }{\displaystyle \sum_{\text{1-gram}\in \bc_1} \text{Count}_{\bc_1}(\text{1-gram})} \\
                & = (\min(\max(0, 0), 1)+\min(\max(0, 0), 1)+\min(\max(0, 0), 1)+\min(\max(0, 0), 1) \\
                & \quad +\min(\max(0, 0), 1) +\min(\max(0, 1), 1)+\min(\max(1, 1), 1)+\min(\max(1, 1), 1) \\ 
                & \quad  +\min(\max(1, 1), 1))/9 \\
                & = (0+0+0+0+0+1+1+1+1)/9 \\
                & = 4/9
            \end{align*}
            \begin{align*}
                p_2 = 
                & = \frac{ \displaystyle \sum_{\text{2-gram} \in \bc} \min \bigg( \max_{i=1,2} \text{Count}_{\br_i}(\text{2-gram}), \enspace \text{Count}_{\bc_1}(\text{2-gram}) \bigg) }{\displaystyle \sum_{\text{2-gram}\in \bc_1} \text{Count}_{\bc_1}(\text{2-gram})} \\
                & = (\min(\max(0, 0), 1)+\min(\max(0, 0), 1)+\min(\max(0, 0), 1)+\min(\max(0, 0), 1) \\
                & \quad + \min(\max(0, 0), 1) + \min(\max(0, 1), 1) + \min(\max(0, 1), 1) + \min(\max(0, 1), 1)) / 8 \\
                & = (0+0+0+0+0+1+1+1)/8 \\
                & = 3/8
            \end{align*}
            Secondly, compute the \textit{brevity penalty} BP.We have $len(c_1) = 9$ and $len(r) = len(r_2)=6$. So $len(c_1) \ge len(r)$. Thus:
            \[BP_1=1\]
            Lastly, the BLEU score for $c_1$ is:
            \begin{align*}
                BLEU_1
                & = BP_1 \times \exp \Big( \sum_{n=1}^2 \lambda_n \log p_n \Big) \\
                & = 1 \times \exp(0.5 \log \frac{4}{9} + 0.5 \log \frac{3}{8}) \\
                & = \sqrt{\frac{1}{6}} \\
                & \approx  0.408
            \end{align*}

            Same as above, for $c_2$:
            \begin{align*}
                p_1
                & = \frac{ \displaystyle \sum_{\text{1-gram} \in \bc} \min \bigg( \max_{i=1,2} \text{Count}_{\br_i}(\text{1-gram}), \enspace \text{Count}_{\bc_2}(\text{1-gram}) \bigg) }{\displaystyle \sum_{\text{1-gram}\in \bc_2} \text{Count}_{\bc_2}(\text{1-gram})} \\
                & = (\min(\max(1, 1), 1)+\min(\max(2, 0), 1)+\min(\max(1, 0), 1)+\min(\max(1, 1), 1) \\
                & \quad +\min(\max(1, 1), 1) +\min(\max(2, 0), 1))/9 \\
                & = (1+1+1+1+1+1)/6 \\
                & = 1
            \end{align*}
            \begin{align*}
                p_2 = 
                & = \frac{ \displaystyle \sum_{\text{2-gram} \in \bc} \min \bigg( \max_{i=1,2} \text{Count}_{\br_i}(\text{2-gram}), \enspace \text{Count}_{\bc_2}(\text{2-gram}) \bigg) }{\displaystyle \sum_{\text{2-gram}\in \bc_2} \text{Count}_{\bc_2}(\text{2-gram})} \\
                & = (\min(\max(0, 0), 1)+\min(\max(1, 0), 1)+\min(\max(1, 0), 1)+\min(\max(0, 1), 1) \\
                & \quad + \min(\max(0, 0), 1)) / 5 \\
                & = (0+1+1+1+0)/5 \\
                & = 3/5
            \end{align*}
            $len(c_2) = 6$ and $len(r) = len(r_2) = 6$. So $len(c_2) = len(r)$. So:
            \[BP_2=1\]
            \begin{align*}
                BLEU_2
                & = BP_2 \times \exp \Big( \sum_{n=1}^2 \lambda_n \log p_n \Big) \\
                & = 1 \times \exp(0.5 \log 1 + 0.5 \log \frac{3}{5}) \\
                & = \sqrt{\frac{3}{5}} \\
                & \approx  0.775
            \end{align*}

            According to the BLUE score, $c_2$ is considered better. However, I think $c_1$ may be the better one because of its frequency and precision.
        }
        
        \subpart[5] Our hard drive was corrupted and we lost Reference Translation $\br_1$. Please recompute BLEU scores for $\bc_1$ and $\bc_2$, this time with respect to $\br_2$ only. Which of the two NMT translations now receives the higher BLEU score? Do you agree that it is the better translation?
        
        \ifans{For $\bc_1$
        \begin{align*}
            p_1
            & = \frac{ \displaystyle \sum_{\text{1-gram} \in \bc} \min \bigg( \text{Count}_{\br_2}(\text{1-gram}), \enspace \text{Count}_{\bc_1}(\text{1-gram}) \bigg) }{\displaystyle \sum_{\text{1-gram}\in \bc_1} \text{Count}_{\bc_1}(\text{1-gram})} \\
            & = (\min(0, 1)+\min(0, 1)+\min(0, 1)+\min(0, 1) \\
            & \quad +\min(0, 1) +\min(1, 1)+\min(1, 1)+\min(1, 1) \\ 
            & \quad  +\min(1, 1))/9 \\
            & = (0+0+0+0+0+1+1+1+1)/9 \\
            & = 4/9
        \end{align*}
        \begin{align*}
            p_2 = 
            & = \frac{ \displaystyle \sum_{\text{2-gram} \in \bc} \min \bigg(\text{Count}_{\br_2}(\text{2-gram}), \enspace \text{Count}_{\bc_1}(\text{2-gram}) \bigg) }{\displaystyle \sum_{\text{2-gram}\in \bc_1} \text{Count}_{\bc_1}(\text{2-gram})} \\
            & = (\min(0, 1)+\min(0, 1)+\min(0, 1)+\min(0, 1) \\
            & \quad + \min(0, 1) + \min(1, 1) + \min(1, 1) + \min(1, 1)) / 8 \\
            & = (0+0+0+0+0+1+1+1)/8 \\
            & = 3/8
        \end{align*}
        \[BP_1=1\]
        \begin{align*}
            BLEU_1
            & = BP_1 \times \exp \Big( \sum_{n=1}^2 \lambda_n \log p_n \Big) \\
            & = 1 \times \exp(0.5 \log \frac{4}{9} + 0.5 \log \frac{3}{8}) \\
            & = \sqrt{\frac{1}{6}} \\
            & \approx  0.408
        \end{align*}

        For $\bc_2$
        \begin{align*}
            p_1
            & = \frac{ \displaystyle \sum_{\text{1-gram} \in \bc} \min \bigg( \text{Count}_{\br_2}(\text{1-gram}), \enspace \text{Count}_{\bc_2}(\text{1-gram}) \bigg) }{\displaystyle \sum_{\text{1-gram}\in \bc_2} \text{Count}_{\bc_2}(\text{1-gram})} \\
            & = (\min(1, 1)+\min(0, 1)+\min(0, 1)+\min(1, 1) \\
            & \quad +\min(1, 1) +\min(0, 1))/9 \\
            & = (1+0+0+1+1+0)/6 \\
            & = 1/2
        \end{align*}
        \begin{align*}
            p_2 = 
            & = \frac{ \displaystyle \sum_{\text{2-gram} \in \bc} \min \bigg( \text{Count}_{\br_2}(\text{2-gram}), \enspace \text{Count}_{\bc_2}(\text{2-gram}) \bigg) }{\displaystyle \sum_{\text{2-gram}\in \bc_2} \text{Count}_{\bc_2}(\text{2-gram})} \\
            & = (\min(0, 1)+\min(0, 1)+\min(0, 1)+\min(1, 1) \\
            & \quad + \min(0, 1)) / 5 \\
            & = (0+0+0+1+0)/5 \\
            & = 1/5
        \end{align*}
        \[BP_2=1\]
        \begin{align*}
            BLEU_2
            & = BP_2 \times \exp \Big( \sum_{n=1}^2 \lambda_n \log p_n \Big) \\
            & = 1 \times \exp(0.5 \log \frac{1}{2}+ 0.5 \log \frac{1}{5}) \\
            & = \sqrt{\frac{1}{10}} \\
            & \approx  0.316
        \end{align*}
        Maybe this is same as the truth that $\bc_2$ is the better translation.
        }
        
        \subpart[2] Due to data availability, NMT systems are often evaluated with respect to only a single reference translation. Please explain (in a few sentences) why this may be problematic. In your explanation, discuss how the BLEU score metric assesses the quality of NMT translations when there are multiple reference transitions versus a single reference translation.
        
        \ifans{Because there may be many different translations to represent one same meaning. Our NMT translation may happen to be same or totally different to the reference translation. BLUE score takes many different reference translations into consideration, which can partly avoid such errors.}
        
        \subpart[2] List two advantages and two disadvantages of BLEU, compared to human evaluation, as an evaluation metric for Machine Translation. 
        
        \ifans{
        \begin{description}
            \item[Advantage] It takes multiple reference translations into consideration, which can avoid contigency. And, comparaed to human evaluation, it is easy to compute using little humanpower.
            \item[Disadvantage] It may only works well on the corpus level because any zeros in precisions scores woll zero the entire BLUE score. Moreover, it's easy to approach the reference. So researchers may optimize model until BLUE scores reach a high level.
        \end{description}
        }
        
    \end{subparts}
\end{parts}
