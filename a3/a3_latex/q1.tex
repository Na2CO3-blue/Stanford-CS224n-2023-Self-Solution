\titledquestion{Machine Learning \& Neural Networks}[8]

\begin{parts}

    
    \part[4] Adam Optimizer\newline
        Recall the standard Stochastic Gradient Descent update rule:
        \alns{
            	\btheta_{t+1} &\gets \btheta_t - \alpha \nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t)
        }
        where $t+1$ is the current timestep, $\btheta$ is a vector containing all of the model parameters, ($\btheta_t$ is the model parameter at time step $t$, and $\btheta_{t+1}$ is the model parameter at time step $t+1$), $J$ is the loss function, $\nabla_{\btheta} J_{\text{minibatch}}(\btheta)$ is the gradient of the loss function with respect to the parameters on a minibatch of data, and $\alpha$ is the learning rate.
        Adam Optimization\footnote{Kingma and Ba, 2015, \url{https://arxiv.org/pdf/1412.6980.pdf}} uses a more sophisticated update rule with two additional steps.\footnote{The actual Adam update uses a few additional tricks that are less important, but we won't worry about them here. If you want to learn more about it, you can take a look at: \url{http://cs231n.github.io/neural-networks-3/\#sgd}}
            
        \begin{subparts}

            \subpart[2]First, Adam uses a trick called {\it momentum} by keeping track of $\bm$, a rolling average of the gradients:
                \alns{
                	\bm_{t+1} &\gets \beta_1\bm_{t} + (1 - \beta_1)\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \\
                	\btheta_{t+1} &\gets \btheta_t - \alpha \bm_{t+1}
                }
                where $\beta_1$ is a hyperparameter between 0 and 1 (often set to  0.9). Briefly explain in 2--4 sentences (you don't need to prove mathematically, just give an intuition) how using $\bm$ stops the updates from varying as much and why this low variance may be helpful to learning, overall.
                \begin{mdframed}[backgroundcolor=gray!10]\begin{answer}
                By using $\bm$, when gradient and $\bm$ are in the same why, the update value will be larger than that we didn't use it. This can accelerate the optimizing progress. If the itms are different in signals, the update value in the end will be smaller, which can avoid varying as much. Thus, this low variance is useful to learning.
                \end{answer} \end{mdframed}
            \subpart[2] Adam extends the idea of {\it momentum} with the trick of {\it adaptive learning rates} by keeping track of  $\bv$, a rolling average of the magnitudes of the gradients:
                \alns{
                	\bm_{t+1} &\gets \beta_1\bm_{t} + (1 - \beta_1)\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \\
                	\bv_{t+1} &\gets \beta_2\bv_{t} + (1 - \beta_2) (\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \odot \nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t)) \\
                	\btheta_{t+1} &\gets \btheta_t - \alpha \bm_{t+1} / \sqrt{\bv_{t+1}}
                }
                where $\odot$ and $/$ denote elementwise multiplication and division (so $\bz \odot \bz$ is elementwise squaring) and $\beta_2$ is a hyperparameter between 0 and 1 (often set to  0.99). Since Adam divides the update by $\sqrt{\bv}$, which of the model parameters will get larger updates?  Why might this help with learning?
                \begin{mdframed}[backgroundcolor=gray!10]\begin{answer}
                    Because of the deviation, the former model will get larger updates.\\
                    By Deviding determined matrix $\sqrt{\bv}$, we can change the learing rate from $\alpha$ to a matrix $\alpha / \sqrt{\bv}$. Considering every parameters are different in convergence speed, by doing these, we can assign different learing rates determined by gradient for different parameters automatically.\\
                    In the beginging, the learning rate will be big and closed to the optimization, the rate will be smaller adpatively.
                \end{answer} \end{mdframed}
        \end{subparts}
        
        
            \part[4] 
            Dropout\footnote{Srivastava et al., 2014, \url{https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf}} is a regularization technique. During training, dropout randomly sets units in the hidden layer $\bh$ to zero with probability $p_{\text{drop}}$ (dropping different units each minibatch), and then multiplies $\bh$ by a constant $\gamma$. We can write this as:
                \alns{
                	\bh_{\text{drop}} = \gamma \bd \odot \bh
                }
                where $\bd \in \{0, 1\}^{D_h}$ ($D_h$ is the size of $\bh$)
                is a mask vector where each entry is 0 with probability $p_{\text{drop}}$ and 1 with probability $(1 - p_{\text{drop}})$. $\gamma$ is chosen such that the expected value of $\bh_{\text{drop}}$ is $\bh$:
                \alns{
                	\mathbb{E}_{p_{\text{drop}}}[\bh_\text{drop}]_i = h_i \text{\phantom{aaaa}}
                }
                for all $i \in \{1,\dots,D_h\}$. 
            \begin{subparts}
            \subpart[2]
                What must $\gamma$ equal in terms of $p_{\text{drop}}$? Briefly justify your answer or show your math derivation using the equations given above.
                \begin{mdframed}[backgroundcolor=gray!10]\begin{answer}
                    Consider the expected value $\bh_{\text{drop}}$
                    
                    \begin{align*}
                        \mathbb{E}_{p_{\text{drop}}}[\bh_\text{drop}]_i
                        & = \mathbb{E}_{p_{\text{drop}}}[\gamma \bd \odot \bh]_i \\
                        & = \gamma \mathbb{E}_{p_{\text{drop}}}[\bd]_i \odot \bh \\
                        & = \gamma (1 - p_{\text{drop}}) \cdot 1 \cdot \bh + \gamma p_{\text{drop}} \cdot 0 \cdot \bh \\
                        & = \gamma (1 - p_{\text{drop}}) \cdot \bh
                    \end{align*}

                    To ensure the expected value are common after dropout. We have:

                    \begin{align*}
                        \gamma (1 - p_{\text{drop}}) \cdot \bh & = \bh \\
                        \gamma & = \frac{1}{1 - p_{\text{drop}}}
                    \end{align*}

                    Therefore, $\gamma$ must equal to $\frac{1}{1 - p_{\text{drop}}}$ in terms of $p_{\text{drop}}$.
                \end{answer} \end{mdframed}
            \subpart[2] Why should dropout be applied during training? Why should dropout \textbf{NOT} be applied during evaluation? (Hint: it may help to look at the paper linked above in the write-up.) 
                \begin{mdframed}[backgroundcolor=gray!10]\begin{answer}
                    During training, random dropout trains exponentially many smaller networks at once and averaging over their predictions, which breaks up co-adaptations builded up working for the training data but not generalized to unseen data. Therefore, dropout improve neural networks by reducing overfitting. \\
                    However, during evalution, what we what is using all of the features we learned to make the most precise prediction. So dropout are useless. Moreover, if we apply dropout these time, because of the random choice, we may got rather different outputs of same input during many trials.
                \end{answer} \end{mdframed}
        \end{subparts}


\end{parts}
